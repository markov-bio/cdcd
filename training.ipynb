{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the TinyStories dataset\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  # or any suitable tokenizer\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "# Preprocess the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])  # Remove original text, keep only tokens\n",
    "tokenized_datasets.set_format(\"torch\")  # Set format to PyTorch tensors\n",
    "\n",
    "vocab_size=tokenizer.vocab_size+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from continous_diffusion.diffusion import Diffusion\n",
    "from continous_diffusion.model import TransformerModel\n",
    "from continous_diffusion.loss import Loss\n",
    "from continous_diffusion.embedding import Embedder\n",
    "from continous_diffusion.scheduling import CauchySchedule\n",
    "from continous_diffusion.conditioning import TimeConditioning\n",
    "\n",
    "embed_dim=256\n",
    "num_heads=4\n",
    "cond_dim=16\n",
    "n_blocks=4\n",
    "\n",
    "dit=TransformerModel(embed_dim,num_heads,cond_dim,n_blocks)\n",
    "embedder=Embedder(vocab_size,embed_dim)\n",
    "schedule=CauchySchedule(0.01,20,1,1,math.log(vocab_size),0)\n",
    "loss=Loss(embedder,schedule)\n",
    "conditioning=TimeConditioning(cond_dim,cond_dim)\n",
    "model=Diffusion(dit,loss,conditioning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512, 256])\n",
      "Loss: 1.9408994913101196\n",
      "torch.Size([8, 512, 256])\n",
      "Loss: 3.1746482849121094\n",
      "torch.Size([8, 512, 256])\n",
      "Loss: 2.605680465698242\n",
      "torch.Size([8, 512, 256])\n",
      "Loss: 3.8398118019104004\n",
      "torch.Size([8, 512, 256])\n",
      "Loss: 3.7289814949035645\n",
      "torch.Size([8, 512, 256])\n",
      "Loss: 2.976043701171875\n",
      "torch.Size([8, 512, 256])\n",
      "Loss: 2.5450997352600098\n",
      "torch.Size([8, 512, 256])\n",
      "Loss: 4.529530048370361\n",
      "torch.Size([8, 512, 256])\n",
      "Loss: 1.8267449140548706\n",
      "torch.Size([8, 512, 256])\n",
      "Loss: 1.8130924701690674\n",
      "torch.Size([8, 512, 256])\n",
      "Loss: 3.344564199447632\n",
      "torch.Size([8, 512, 256])\n",
      "Loss: 3.671360969543457\n",
      "torch.Size([8, 512, 256])\n",
      "Loss: 1.5351699590682983\n",
      "torch.Size([8, 512, 256])\n",
      "Loss: 1.085735559463501\n",
      "torch.Size([8, 512, 256])\n",
      "Loss: 2.6575260162353516\n",
      "torch.Size([8, 512, 256])\n",
      "Loss: 3.197154998779297\n",
      "torch.Size([8, 512, 256])\n"
     ]
    }
   ],
   "source": [
    "# DataLoader\n",
    "train_loader = DataLoader(tokenized_datasets[\"train\"], batch_size=8, shuffle=True)\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(1):  # Define num_epochs\n",
    "    for batch in train_loader:\n",
    "        tokens = batch['input_ids']\n",
    "        x,sigma=model.make_sample(tokens)\n",
    "        print(x.shape)\n",
    "        prediction=model(x,sigma)\n",
    "        # Forward pass\n",
    "        loss = model.loss(tokens,prediction,sigma)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()  # Define your optimizer\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log, print, or save as needed\n",
    "        print(f\"Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
